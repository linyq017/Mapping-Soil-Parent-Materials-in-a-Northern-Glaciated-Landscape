from sklearn.model_selection import GridSearchCV
import numpy as np
import pandas as pd
import os
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV
from pprint import pprint
from sklearn.preprocessing import LabelEncoder 
from sklearn.preprocessing import MinMaxScaler
from sklearn.neural_network import MLPClassifier

#import trainingdata
os.chdir('data')
df_train = pd.read_csv('TrainingData.txt', sep = '\t', decimal = ',')

#slice x training and y predicting variables
var_columns = [c for c in df_train.columns if c not in ['Process', 'Proesskod', 'soil']]
x = df_train.loc[:,var_columns]
y = df_train.loc[:,'Proesskod']

#encode y label to start from 0 xgboost requirement 
le = LabelEncoder()
y_new = le.fit_transform(y)

#split data into 70% training and 30% testing 
x_train, x_test, y_train, y_test = train_test_split(x, y_new, test_size = 0.3, random_state=42, stratify = y)

#standardize the data to a predefined range
scaler = MinMaxScaler()
x_train_NN = scaler.fit_transform(x_train)
x_test_NN = scaler.fit_transform(x_test)

# define the grid search parameters
parameter_space = {
    'hidden_layer_sizes': [(100,50,20,5), (200, 50, 10), (100, 50, 10)],
    'activation': ['tanh', 'relu'],
    'solver': ['adam'],
    'alpha': [0.1, 0.2],
    'learning_rate': ['adaptive'],
    'max_iter': [500,1000,1500 ]
}

#define nn model
nn=MLPClassifier()
#grid search 
clf = GridSearchCV(nn, parameter_space, n_jobs=-1, cv=3)
clf.fit(x_train, y_train)
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))
